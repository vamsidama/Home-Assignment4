# -*- coding: utf-8 -*-
"""Home-Assignment4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mr5L4HW7bl8meyHJDzhdIVnceyla9zEC
"""

# Q1 NLP Preprocessing Pipeline

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import os

def download_nltk_resources  (resources):
    """
    Downloads required NLTK resources with error handling.
    Args:
        resources (list): List of NLTK resource names to download
    Returns:
        bool: True if all downloads succeed, False otherwise
    """
    try:
        for resource in resources:
            print(f"Downloading NLTK resource: {resource}")
            nltk.download(resource, quiet=True)
        return True
    except Exception as e:
        print(f"Error downloading NLTK resources: {e}")
        return False

def nlp_preprocessing(sentence):
    """
    Performs NLP preprocessing: tokenization, stopword removal, and stemming.
    Args:
        sentence (str): Input sentence to process
    Returns:
        None: Prints original tokens, tokens without stopwords, and stemmed words
    """
    # Step 1: Tokenize the sentence
    tokens = word_tokenize(sentence)
    print("Original Tokens:", tokens)

    # Step 2: Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens_no_stopwords = [word for word in tokens if word.lower() not in stop_words]
    print("Tokens Without Stopwords:", tokens_no_stopwords)

    # Step 3: Apply stemming
    stemmer = PorterStemmer()
    stemmed_words = [stemmer.stem(word) for word in tokens_no_stopwords]
    print("Stemmed Words:", stemmed_words)

# Download required NLTK resources
required_resources = ['punkt', 'punkt_tab', 'stopwords']
if not download_nltk_resources(required_resources):
    print("Failed to download NLTK resources. Please check your internet connection and try again.")
    exit(1)

# Test the function
sentence = "NLP techniques are used in virtual assistants like Alexa and Siri."
nlp_preprocessing(sentence)

"""1. What is the difference between stemming and lemmatization? Provide examples with the word “running.”

 Answer: Stemming: Reduces words to their root form by removing suffixes, often using heuristic rules. It may produce non-words. Example: "running" → "run" (Porter Stemmer).
Lemmatization: Reduces words to their base form (lemma) using a dictionary, ensuring valid words. Example: "running" → "run" (same as stemming, but considers context, e.g., verb vs. noun).

 Difference: Stemming is faster but less accurate; lemmatization is slower but more precise, preserving meaning.


2. Why might removing stop words be useful in some NLP tasks, and when might it actually be harmful?

 Answer: Useful: Stop words (e.g., "the", "is") are common and often carry little meaning in tasks like text classification or topic modeling. Removing them reduces noise, lowers dimensionality, and improves model efficiency.
Harmful: In tasks like sentiment analysis or machine translation, stop words provide context or grammatical structure. Removing them may alter meaning, e.g., "not good" vs. "good" in sentiment analysis.
"""

# Q2 Named Entity Recognition with SpaCy

import spacy
import subprocess
import sys

def install_spacy_model(model_name="en_core_web_sm"):
    """
    Installs the specified SpaCy model if not already installed.
    Args:
        model_name (str): Name of the SpaCy model to install
    Returns:
        bool: True if model is installed or already exists, False otherwise
    """
    try:
        spacy.load(model_name)
        print(f"Model {model_name} is already installed.")
        return True
    except OSError:
        print(f"Installing SpaCy model: {model_name}")
        try:
            subprocess.check_call([sys.executable, "-m", "spacy", "download", model_name])
            return True
        except subprocess.CalledProcessError as e:
            print(f"Error installing {model_name}: {e}")
            return False

def extract_entities(sentence):
    """
    Extracts named entities from a sentence using SpaCy.
    Args:
        sentence (str): Input sentence to process
    Returns:
        None: Prints entity text, label, start, and end positions
    """
    # Load SpaCy English model
    nlp = spacy.load("en_core_web_sm")

    # Process the sentence
    doc = nlp(sentence)

    # Print named entities
    for ent in doc.ents:
        print(f"Entity: {ent.text}, Label: {ent.label_}, Start: {ent.start_char}, End: {ent.end_char}")

# Install required SpaCy model
if not install_spacy_model("en_core_web_sm"):
    print("Failed to install SpaCy model 'en_core_web_sm'. Please install it manually using: python -m spacy download en_core_web_sm")
    sys.exit(1)

# Test the function
sentence = "Barack Obama served as the 44th President of the United States and won the Nobel Peace Prize in 2009."
extract_entities(sentence)

"""1. How does NER differ from POS tagging in NLP?

 Answer: NER (Named Entity Recognition): Identifies and classifies named entities (e.g., "Barack Obama" as PERSON) in text, focusing on specific proper nouns or phrases.
POS Tagging (Part-of-Speech Tagging): Assigns grammatical categories (e.g., noun, verb) to each word in a sentence, focusing on syntactic roles.
Difference: NER is semantic (entity-focused), while POS tagging is syntactic (grammar-focused).

2. Describe two applications that use NER in the real world.

 Answer: Financial News: NER extracts entities like company names (e.g., "Apple") or stock tickers from news articles to track market trends or automate trading decisions.
Search Engines: NER identifies entities like people or locations in queries (e.g., "restaurants in New York") to provide relevant, entity-specific results.
"""

# Q3 Scaled Dot-Product Attention

import numpy as np
from scipy.special import softmax

def scaled_dot_product_attention(Q, K, V):
    """
    Implements scaled dot-product attention mechanism.
    Args:
        Q (np.array): Query matrix
        K (np.array): Key matrix
        V (np.array): Value matrix
    Returns:
        None: Prints attention weights and final output
    """
    # Get dimension of keys
    d_k = K.shape[-1]

    # Compute dot product of Q and K^T
    dot_product = np.dot(Q, K.T)

    # Scale by sqrt(d_k)
    scaled_dot_product = dot_product / np.sqrt(d_k)

    # Apply softmax to get attention weights
    attention_weights = softmax(scaled_dot_product, axis=-1)
    print("Attention Weights:\n", attention_weights)

    # Compute final output by multiplying attention weights with V
    output = np.dot(attention_weights, V)
    print("Final Output:\n", output)

# Test inputs
Q = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])
K = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])
V = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])

# Run the function
scaled_dot_product_attention(Q, K, V)

"""1. Why do we divide the attention score by √d in the scaled dot-product attention formula?

 Answer: Dividing by √d (where d is the key dimension) prevents large dot-product values, which can lead to small gradients during backpropagation due to softmax saturation. Scaling stabilizes training and ensures attention weights are well-distributed.

2. How does self-attention help the model understand relationships between words in a sentence?

 Answer: Self-attention computes attention scores between all pairs of words, allowing the model to capture dependencies (e.g., "The cat... chased" links "cat" and "chased") regardless of their distance in the sentence. This helps understand context and relationships dynamically.
"""

# Q4 Sentiment Analysis using HuggingFace Transformers

import os
import sys
from transformers import pipeline

# Suppress TensorFlow warnings and CUDA-related errors
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Disable oneDNN optimizations
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'   # Suppress TensorFlow logging (0 = all, 1 = filter INFO, 2 = filter WARNING, 3 = filter ERROR)

def sentiment_analysis(sentence):
    """
    Performs sentiment analysis using HuggingFace transformers.
    Args:
        sentence (str): Input sentence to analyze
    Returns:
        None: Prints sentiment label and confidence score
    """
    try:
        # Load pre-trained sentiment analysis pipeline with explicit model
        classifier = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")

        # Analyze the sentence
        result = classifier(sentence)[0]

        # Print results
        print(f"Sentiment: {result['label']}")
        print(f"Confidence Score: {result['score']:.4f}")

    except Exception as e:
        print(f"Error during sentiment analysis: {e}")
        sys.exit(1)

# Test the function
sentence = "Despite the high price, the performance of the new MacBook is outstanding."
sentiment_analysis(sentence)

"""1. What is the main architectural difference between BERT and GPT? Which uses an encoder and which uses a decoder?

 Answer: BERT (Bidirectional Encoder Representations from Transformers): Uses an encoder-only architecture, processing input bidirectionally for tasks like classification or NER.
GPT (Generative Pre-trained Transformer): Uses a decoder-only architecture, processing input unidirectionally (left-to-right) for generative tasks like text generation.
Difference: BERT’s encoder captures context from both directions; GPT’s decoder generates text sequentially.
2. Explain why using pre-trained models (like BERT or GPT) is beneficial for NLP applications instead of training from scratch.

 Answer: Pre-trained models are trained on large datasets, learning general language patterns (e.g., grammar, semantics). Fine-tuning them for specific tasks is faster, requires less data, and achieves better performance than training from scratch, which is computationally expensive and data-intensive.
"""